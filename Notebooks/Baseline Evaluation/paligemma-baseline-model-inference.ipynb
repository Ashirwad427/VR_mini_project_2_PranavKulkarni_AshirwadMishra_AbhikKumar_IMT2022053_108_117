{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11533708,"sourceType":"datasetVersion","datasetId":7233922},{"sourceId":11788640,"sourceType":"datasetVersion","datasetId":7401994},{"sourceId":11788775,"sourceType":"datasetVersion","datasetId":7402088}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PaliGemma2 - 3Billion BASELINE EVALUATION (NO FINE-TUNING)","metadata":{}},{"cell_type":"code","source":"# Dependencies\n!pip install --upgrade --quiet transformers sentence-transformers bert-score pandas tqdm datasets accelerate peft bitsandbytes pillow trl --no-deps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing modules\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nimport os\nimport numpy as np\nfrom huggingface_hub import login\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### To use the Paligemma model, we must first create a HuggingFace ID, accept their T&C to use this model and then create an acess token. Using that access token, we can load the model and use it for either inferencing or fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"### The following function 'paligemma_load' logs into HuggingFace using the Access Token and then loads the model - 'google/paligemma-3b-pt-224' and its processor. It then returns these two. We have used lower precision, i.e. \"float16\" to accomodate memory constraints.","metadata":{}},{"cell_type":"code","source":"# Load function\ndef paligemma_load():\n    login('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') # Login to huggingface\n    \n    model = PaliGemmaForConditionalGeneration.from_pretrained(\n            \"google/paligemma-3b-pt-224\",\n            torch_dtype=torch.float16,\n            device_map='auto',\n            revision=\"float16\",\n        ).eval()\n    processor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\",use_fast=True)\n    return model,processor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The following function 'paligemma_inference' takes the output model and processor of the previous function and also takes the Image Path and Question from the row of our input dataframe. Using these, it passes them as inputs to the Paligemma model along with a prompt - \"Answer the question in exactly one word\". This function, then, returns the output given by the decoder of this model.","metadata":{}},{"cell_type":"code","source":"# Inference function\ndef paligemma_inference(img_path,question_text,model,processor):\n    image = Image.open(img_path)\n    text = f'<image> Answer the question in exactly one word:{question_text}'\n    model_inputs = processor(text=text,\n                             images=image,\n                             return_tensors=\"pt\").to(model.device)\n    \n    input_len = model_inputs[\"input_ids\"].shape[-1]\n    \n    with torch.inference_mode():\n        generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n        generation = generation[0][input_len:]\n        decoded = processor.decode(generation, skip_special_tokens=True)\n\n    return decoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initially, we thought of splitting the data into train:test:val::80:10:10  and use the train and val for fine-tuning and only test for getting the baseline inference. Later, we changed this by using both test and val for evaluating both baseline and fine-tuned.\n### Thus, below is code which concatenates the two dataframes as is.","metadata":{}},{"cell_type":"code","source":"# Loading datasets and preparing them\ndf  = pd.read_csv(\"/kaggle/input/vqa-test-dataset/blip_vqa_test.csv\")\ndf1 = pd.read_csv(\"/kaggle/input/vqa-val-dataset/blip_vqa_val.csv\")\n\n# Concatenate df1 below df and reset the index\ndf = pd.concat([df, df1], ignore_index=True)\n\nprint(df['main_image_id'].size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now, we load the model by invoking its respective function. Next, we run the inference loop on our dataframe wherein we call the previously defined inference function and store the prediction and ground-truths. At the end, we convert it to a Dataframe to be used for evaluation","metadata":{}},{"cell_type":"code","source":"model, processor = paligemma_load()\n\nimage_root = \"/kaggle/input/abo-small/images/small\"\n\nresults = []\nfor idx, row in df.iterrows():\n    \n    rel_path  = row['image_path']\n    \n    img_path  = os.path.join(image_root, rel_path).replace(\"\\\\\",\"/\")\n    \n    question  = row['question']\n    gt_answer = row['answer']\n    \n    pred = paligemma_inference(\n        img_path=img_path,\n        question_text=question,\n        model=model,\n        processor=processor\n    )\n    \n    results.append({\n        \"ground_truth\": gt_answer,\n        \"prediction\":   pred\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df.head(20))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### While running the evaluation metric at first, we found that some rows had no predictions at all, which led to a type-error between str and null type. Thus, we fill the null values with empty string to resolve this.","metadata":{}},{"cell_type":"code","source":"# before computing any metrics\ny_true = results_df['ground_truth'].fillna('').astype(str)\ny_pred = results_df['prediction'].fillna('').astype(str)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = sorted(set(y_true).union(set(y_pred)))  # all strings now","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EVALUATION","metadata":{}},{"cell_type":"markdown","source":"### We use 3 evaluation metrics for our entire project, find the section \"Evaluation Metrics\" in the report for the description and purpose of the same:¶\n### - Exact Match Accuracy\n### - BERTScore\n### - Semantic Cosine Similarity","metadata":{}},{"cell_type":"code","source":"# Calculating exact match accuracy score\nfrom sklearn.metrics import accuracy_score\n\nacc = accuracy_score(y_true, y_pred)\nprint(f\"Exact-match Accuracy: {acc*100:.4f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### This value comes out to be 0.06% which is exceptionally poor","metadata":{}},{"cell_type":"code","source":"# Calculating BERTScore\nfrom bert_score import score as bert_score\n\nP, R, F1 = bert_score(\n    cands=y_pred.tolist(),\n    refs =y_true.tolist(),\n    lang =\"en\",\n    rescale_with_baseline=True\n)\nprint(f\"\\nBERTScore →   F1: {F1.mean():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The BERTScore F1 values shows the correctness metric as - 70.88%","metadata":{}},{"cell_type":"code","source":"# Calculating Semantic Cosine Similarity score\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nst_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nemb_true = st_model.encode(y_true.tolist(), convert_to_numpy=True)\nemb_pred = st_model.encode(y_pred.tolist(), convert_to_numpy=True)\n\n# cosine_similarity gives an NxN matrix; we take its diagonal\ncos_mat     = cosine_similarity(emb_true, emb_pred)\ncos_scores  = np.diag(cos_mat)\nprint(f\"\\nAvg. semantic cosine similarity: {cos_scores.mean()*100:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The average cosine similarity evaluates the model preciseness to be - 69.47%","metadata":{}}]}