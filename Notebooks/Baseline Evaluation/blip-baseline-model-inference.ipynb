{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11533708,"sourceType":"datasetVersion","datasetId":7233922},{"sourceId":11659600,"sourceType":"datasetVersion","datasetId":7316978},{"sourceId":11754662,"sourceType":"datasetVersion","datasetId":7379445},{"sourceId":11788640,"sourceType":"datasetVersion","datasetId":7401994},{"sourceId":11788775,"sourceType":"datasetVersion","datasetId":7402088}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Blip Baseline evaluation (NO FINE TUNING)","metadata":{}},{"cell_type":"code","source":"# Dependencies\n!pip install --upgrade --quiet transformers bert-score pandas tqdm datasets accelerate peft bitsandbytes pillow trl --no-deps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# importing modules\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom bert_score import score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Uploaded the required datasets for evaluating the baseline model:\n#### - abo-small\n#### - vqa-test-dataset\n#### - vqa-val-dataset\n\n  ### The sizes of both the curated datasets is - ~1900 products with 4-5 question-answer pairs for each image.","metadata":{}},{"cell_type":"code","source":"\n# Load your data\ndf  = pd.read_csv(\"/kaggle/input/vqa-test-dataset/blip_vqa_test.csv\")\ndf1 = pd.read_csv(\"/kaggle/input/vqa-val-dataset/blip_vqa_val.csv\")\n\n# Concatenate df1 below df and reset the index\ndf = pd.concat([df, df1], ignore_index=True)\n\n# (Optional) Quick sanity check\nprint(f\"Combined dataframe shape: {df.shape}\")\nprint(df['main_image_id'].size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The pre-trained BLIP model is loaded and stored from the tranformer library","metadata":{}},{"cell_type":"code","source":"# Initializing the BLIP baseline model\ndevice    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel     = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now we use the dataset rows and extract the image_path, concatenate it with image_root and pass it as imafe input to the model. Along with this, we pass the \"question\" for that row, generate the output and store it.","metadata":{}},{"cell_type":"code","source":"# Running the inference loop on the dataset using the pre-trained model\n# stores the predictions(preds) and ground truths(refs)\npreds = []\nrefs  = []\nimage_root = \"/kaggle/input/abo-small/images/small\" # will be concatenated with image_path\n\nfor _, row in df.iterrows():\n    img_path = os.path.join(image_root, row[\"image_path\"]).replace(\"\\\\\",\"/\")\n    image    = Image.open(img_path).convert(\"RGB\")\n    inputs   = processor(image, row[\"question\"], return_tensors=\"pt\").to(device)\n    out      = model.generate(**inputs)\n    pred     = processor.decode(out[0], skip_special_tokens=True).strip()\n    \n    preds.append(pred)\n    refs.append(str(row[\"answer\"]).strip())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EVALUATION","metadata":{}},{"cell_type":"markdown","source":"### We use 3 evaluation metrics for our entire project, find the section \"Evaluation Metrics\" in the report for the description and purpose of the same:\n#### - Exact Match Accuracy\n#### - BERTScore\n#### - Semantic Cosine Similarity","metadata":{}},{"cell_type":"code","source":"# Computing exact match accuracy\nexact_match = sum(p.lower() == r.lower() for p, r in zip(preds, refs)) / len(preds)\nprint(f\"Exact-Match Accuracy : {exact_match * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### This value comes out to be 41.22 which is quite obvious given the fact that we are evaluating for the \"exact match\" between the predictions and the ground-truths which does not take into account \"case insensitivity\".","metadata":{}},{"cell_type":"code","source":"# Computing BERTScore (F1)\nP, R, F1 = score(preds, refs, lang=\"en\", rescale_with_baseline=True)\nprint(f\"BERTScore F1 :         {F1.mean().item() * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### This value comes out to be 68.53%","metadata":{}},{"cell_type":"code","source":"# Computing Semantic cosine similarity (via SentenceTransformers)\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nst_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nemb_true = st_model.encode(refs, convert_to_numpy=True)\nemb_pred = st_model.encode(preds, convert_to_numpy=True)\n\n# cosine_similarity gives an NxN matrix; we take its diagonal\ncos_mat     = cosine_similarity(emb_true, emb_pred)\ncos_scores  = np.diag(cos_mat)\nprint(f\"\\nAvg. semantic cosine similarity: {cos_scores.mean():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finally, the semantic cosine similarity evaluates the model correctness as 71.90%","metadata":{}}]}